{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#References\" data-toc-modified-id=\"References-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>References</a></span></li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load Data</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preprocessing</a></span></li><li><span><a href=\"#Folds\" data-toc-modified-id=\"Folds-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Folds</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preparation\" data-toc-modified-id=\"Preparation-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Preparation</a></span></li><li><span><a href=\"#Deep-multilabel-model-keras\" data-toc-modified-id=\"Deep-multilabel-model-keras-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Deep multilabel model keras</a></span></li><li><span><a href=\"#Deep-multilabel-model-torch\" data-toc-modified-id=\"Deep-multilabel-model-torch-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Deep multilabel model torch</a></span></li><li><span><a href=\"#Deep-multilabel-CV\" data-toc-modified-id=\"Deep-multilabel-CV-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Deep multilabel CV</a></span></li><li><span><a href=\"#Term-model\" data-toc-modified-id=\"Term-model-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Term model</a></span></li><li><span><a href=\"#Zero-class-prediction-model\" data-toc-modified-id=\"Zero-class-prediction-model-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Zero class prediction model</a></span></li><li><span><a href=\"#Error-class-prediction-model\" data-toc-modified-id=\"Error-class-prediction-model-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Error class prediction model</a></span></li><li><span><a href=\"#Blender-model\" data-toc-modified-id=\"Blender-model-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Blender model</a></span></li></ul></li><li><span><a href=\"#Public-models\" data-toc-modified-id=\"Public-models-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Public models</a></span><ul class=\"toc-item\"><li><span><a href=\"#keras-NN-+PCA-with-Label-smoothing-CV[0.01562]-LB-[0.01859]\" data-toc-modified-id=\"keras-NN-+PCA-with-Label-smoothing-CV[0.01562]-LB-[0.01859]-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>keras NN +PCA with Label smoothing CV[0.01562] LB [0.01859]</a></span></li><li><span><a href=\"#Pytorch-RankGauss-PCA-NN-CV-[0.014572]-LB-[0.01839]\" data-toc-modified-id=\"Pytorch-RankGauss-PCA-NN-CV-[0.014572]-LB-[0.01839]-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Pytorch-RankGauss-PCA-NN CV [0.014572] LB [0.01839]</a></span></li><li><span><a href=\"#MODEL1-CV-[0.01562060391771847]-LB-[0.01833]\" data-toc-modified-id=\"MODEL1-CV-[0.01562060391771847]-LB-[0.01833]-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>MODEL1 CV [0.01562060391771847] LB [0.01833]</a></span></li></ul></li><li><span><a href=\"#Auto-Tuning\" data-toc-modified-id=\"Auto-Tuning-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Auto Tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Error-class\" data-toc-modified-id=\"Error-class-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Error class</a></span></li><li><span><a href=\"#Zero-class\" data-toc-modified-id=\"Zero-class-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Zero class</a></span></li><li><span><a href=\"#Blender\" data-toc-modified-id=\"Blender-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Blender</a></span></li><li><span><a href=\"#Run!\" data-toc-modified-id=\"Run!-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Run!</a></span></li></ul></li><li><span><a href=\"#Final-prediction\" data-toc-modified-id=\"Final-prediction-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Final prediction</a></span></li><li><span><a href=\"#Submission\" data-toc-modified-id=\"Submission-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Submission</a></span></li><li><span><a href=\"#Error-analysis\" data-toc-modified-id=\"Error-analysis-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Error analysis</a></span></li><li><span><a href=\"#Offline-vs-Public\" data-toc-modified-id=\"Offline-vs-Public-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Offline vs Public</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "https://www.kaggle.com/sinamhd9/mechanisms-of-action-moa-tutorial/data\n",
    "\n",
    "scalers: https://www.kaggle.com/liuhdme/moa-competition\n",
    "\n",
    "keras: https://www.kaggle.com/riadalmadani/keras-nn-pca-with-label-smoothing/comments\n",
    "\n",
    "blending: https://www.kaggle.com/c/lish-moa/notebooks?competitionId=19988&sortBy=scoreDescending\n",
    "\n",
    "Classes info:\n",
    "https://docs.google.com/spreadsheets/d/1NVPfqcJKWd-Oes610N-wHMYOKpO2WiH6PU7wFqZyNX8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что можно поделать:\n",
    "1. (+) Модель предсказывающая все нули или нет\n",
    "2. Модель определения termов из названий классов\n",
    "3. Кластеризация + Y-decode\n",
    "4. G,C - autoencoders\n",
    "5. Imbalance\n",
    "6. Class cleaning, только в train!\n",
    "7. (+) Отдельная модель на классы с ошибками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:28:04.582764Z",
     "start_time": "2020-11-29T17:27:59.008709Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f7f52a3e2ed4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mhas_internet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://datadigger.ru'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q mlflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m             )\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# Reset the timeout for the recv() on the socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1251\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;31m# default charset of iso-8859-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\\r\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             )\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Importing useful libraries\n",
    "# pip install jupyter_contrib_nbextensions lightgbm iterative-stratification tensorflow tensorflow-addons mlflow kaggle jupyter hyperopt\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from os.path import isfile\n",
    "from functools import partial\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Adding iterative-stratification \n",
    "# Select add data from the right menu and search for iterative-stratification, then add it to your kernel.\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "sys.path.append('../input/rank-gauss')\n",
    "from gauss_rank_scaler import GaussRankScaler\n",
    "\n",
    "from time import time\n",
    "import datetime\n",
    "import gc\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# ML tools \n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import log_loss\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "# Setting random seeds\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed=42)\n",
    "\n",
    "# Visualization tools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('white')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "import requests\n",
    "has_internet = True\n",
    "try:\n",
    "    r = requests.get('http://datadigger.ru', timeout=3)\n",
    "    !pip install -q mlflow\n",
    "    import mlflow\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://datadigger.ru:5000')\n",
    "\n",
    "except Exception:\n",
    "    has_internet = False\n",
    "\n",
    "def dict_flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            items.extend(dict_flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:28:10.164633Z",
     "start_time": "2020-11-29T17:28:04.584838Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\n",
    "print('train data size', df_train.shape)\n",
    "\n",
    "df_drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\n",
    "print('train drug size', df_drug.shape)\n",
    "\n",
    "df_target_ns = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\n",
    "print('train target nonscored size', df_target_ns.shape)\n",
    "\n",
    "df_target_s = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n",
    "print('train target scored size', df_target_s.shape)\n",
    "\n",
    "df_target_s = df_target_s.merge(df_drug, on=['sig_id'])\n",
    "print('train target scored size', df_target_s.shape)\n",
    "\n",
    "df_test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\n",
    "print('test data size', df_test.shape)\n",
    "\n",
    "df_sample = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\n",
    "print('sample submission size', df_sample.shape)\n",
    "\n",
    "def g_c_features(df):\n",
    "    g_features = [cols for cols in df.columns if cols.startswith('g-')]\n",
    "    c_features = [cols for cols in df.columns if cols.startswith('c-')]\n",
    "    return g_features, c_features\n",
    "\n",
    "def preprocess(df):\n",
    "    df['cp_time'] = df['cp_time'].map({24:1, 48:2, 72:3})\n",
    "    df['cp_dose'] = df['cp_dose'].map({'D1':0, 'D2':1})\n",
    "    df['cp_type'] = df['cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    return df\n",
    "\n",
    "X = preprocess(df_train)\n",
    "X_test = preprocess(df_test)\n",
    "\n",
    "ind_te = X_test[X_test['cp_type']==1].index\n",
    "\n",
    "y = df_target_s.drop('sig_id', axis=1)\n",
    "y0 =  df_target_ns.drop('sig_id', axis=1)\n",
    "\n",
    "print('New data shape', X.shape)\n",
    "\n",
    "def perc_empty(df):\n",
    "    df = df.drop(['drug_id'], axis=1, errors='ignore')\n",
    "    return 100 * (1 - len(df[(df.T != 0).any()]) / len(df))\n",
    "\n",
    "print(\"Scored без класса: {}%\".format(round(perc_empty(y), 3)))\n",
    "print(\"NoScored без класса: {}%\".format(round(perc_empty(y0), 3)))\n",
    "print(\"Нет никакого класса: {}%\".format(round(perc_empty(pd.concat([y, y0], axis=1)), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:28:10.424896Z",
     "start_time": "2020-11-29T17:28:10.167320Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def variance_reduction(X, X_test, **params):\n",
    "    if not params.get('enabled', True):\n",
    "        return X, X_test\n",
    "    thresh = params.get('threshold', 0.8)\n",
    "\n",
    "    columns_to_skip = [\n",
    "        c for c in ['drug_id', 'cp_type', 'cp_time','cp_dose'] \n",
    "        if c in set(X.columns)\n",
    "    ]\n",
    "    cols_num = len(columns_to_skip)\n",
    "    \n",
    "    var_thresh = VarianceThreshold(thresh)\n",
    "    data = X.append(X_test)\n",
    "    try:\n",
    "        data_transformed = var_thresh.fit_transform(data.iloc[:, cols_num:])\n",
    "    except Exception as e:\n",
    "        print(e, str(thresh))\n",
    "        return X, X_test\n",
    "\n",
    "    train_features_transformed = data_transformed[ : X.shape[0]]\n",
    "    test_features_transformed = data_transformed[-X_test.shape[0] : ]\n",
    "\n",
    "    X = pd.DataFrame(\n",
    "        X[columns_to_skip].values.reshape(-1, cols_num),\n",
    "        columns=columns_to_skip\n",
    "    )\n",
    "\n",
    "    X = pd.concat([X, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "    X_test = pd.DataFrame(\n",
    "        X_test[columns_to_skip].values.reshape(-1, cols_num),\n",
    "        columns=columns_to_skip\n",
    "    )\n",
    "\n",
    "    X_test = pd.concat([X_test, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "    return X, X_test\n",
    "\n",
    "def fe_cluster(train, test, **params):\n",
    "    if not params.get('enabled', True):\n",
    "        return train, test\n",
    "    n_clusters_g = params.get('n_clusters_g', 35) \n",
    "    n_clusters_c = params.get('n_clusters_c', 5)\n",
    "    random_state = params.get('seed', 299)\n",
    "    \n",
    "    features_g, features_c = g_c_features(train)\n",
    "    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n",
    "        train_ = train[features].copy()\n",
    "        test_ = test[features].copy()\n",
    "        data = pd.concat([train_, test_], axis = 0)\n",
    "        kmeans = KMeans(n_clusters = n_clusters, random_state = random_state).fit(data)\n",
    "        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n",
    "        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n",
    "        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n",
    "        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "        return train, test\n",
    "    \n",
    "    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n",
    "    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n",
    "    return train, test\n",
    "\n",
    "def quantile_transformer(df, df_test, **params):\n",
    "    random_state = params.get('seed', 42)\n",
    "    g_features, c_features = g_c_features(df)\n",
    "\n",
    "    for col in (g_features + c_features):\n",
    "        transformer = QuantileTransformer(n_quantiles=100, random_state=random_state, output_distribution='normal')\n",
    "        vec_len = len(df[col].values)\n",
    "        raw_vec = df[col].values.reshape(vec_len, 1)\n",
    "        vec_len_test = len(df_test[col].values)\n",
    "        raw_vec_test = df_test[col].values.reshape(vec_len_test, 1)\n",
    "        transformer.fit(raw_vec)\n",
    "        df[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "        df_test[col] = transformer.transform(raw_vec_test).reshape(1, vec_len_test)[0]\n",
    "    return df, df_test\n",
    "\n",
    "def rank_gauss(df, **params):\n",
    "    g_features, c_features = g_c_features(df)\n",
    "    cols_numeric = g_features + c_features\n",
    "    df[cols_numeric] = GaussRankScaler().fit_transform(df[cols_numeric])\n",
    "    return df\n",
    "\n",
    "def standard_scaler(df, df_test, **params):\n",
    "    g_features, c_features = g_c_features(df)\n",
    "    cols_numeric = g_features + c_features\n",
    "    scaler = StandardScaler()\n",
    "    df[cols_numeric] = scaler.fit_transform(df[cols_numeric])\n",
    "    df_test[cols_numeric] = scaler.transform(df_test[cols_numeric])\n",
    "    return df, df_test\n",
    "\n",
    "def fe_cluster_pca(train, test, n_clusters=5, seed = 42):\n",
    "    pca_g_cols = [c for c in train.columns if c.startswith('pca_g-')]\n",
    "    pca_c_cols = [c for c in train.columns if c.startswith('pca_c-')]\n",
    "    train_gpca = train[pca_g_cols]\n",
    "    test_gpca = test[pca_g_cols]\n",
    "    train_cpca = train[pca_c_cols]\n",
    "    test_cpca = test[pca_c_cols]\n",
    "\n",
    "    train_pca=pd.concat((train_gpca,train_cpca),axis=1)\n",
    "    test_pca=pd.concat((test_gpca,test_cpca),axis=1)\n",
    "\n",
    "    data=pd.concat([train_pca,test_pca],axis=0)\n",
    "    kmeans = KMeans(n_clusters = n_clusters, random_state = seed).fit(data)\n",
    "\n",
    "    train[f'clusters_pca'] = kmeans.labels_[:train.shape[0]]\n",
    "    test[f'clusters_pca'] = kmeans.labels_[train.shape[0]:]\n",
    "\n",
    "    train = pd.get_dummies(train, columns = [f'clusters_pca'])\n",
    "    test = pd.get_dummies(test, columns = [f'clusters_pca'])\n",
    "    return train, test\n",
    "\n",
    "def pca_transformer(X, X_test, **params):\n",
    "    # Please see reference 3 for this part\n",
    "    if not params.get('enabled', True):\n",
    "        return X, X_test\n",
    "    random_state = params.get('seed', 42)\n",
    "    n_comp_cells = params.get('n_comp_cells', 0.95)\n",
    "    n_comp_genes = params.get('n_comp_genes', 0.95)\n",
    "#     print(f'pca {n_comp_cells}, {n_comp_genes}') \n",
    "    g_features, c_features = g_c_features(X)\n",
    "\n",
    "    data = pd.concat([pd.DataFrame(X[g_features]), pd.DataFrame(X_test[g_features])])\n",
    "    data2 = (PCA(n_comp_genes, random_state=random_state).fit_transform(data[g_features]))\n",
    "    train2 = data2[:X.shape[0]]\n",
    "    test2 = data2[-X_test.shape[0]:]\n",
    "\n",
    "    train2 = pd.DataFrame(train2, columns=[f'pca_g-{i}' for i in range(data2.shape[1])])\n",
    "    test2 = pd.DataFrame(test2, columns=[f'pca_g-{i}' for i in range(data2.shape[1])])\n",
    "\n",
    "    X = pd.concat((X, train2), axis=1)\n",
    "    X_test = pd.concat((X_test, test2), axis=1)\n",
    "\n",
    "    data = pd.concat([pd.DataFrame(X[c_features]), pd.DataFrame(X_test[c_features])])\n",
    "    data2 = (PCA(n_comp_cells, random_state=random_state).fit_transform(data[c_features]))\n",
    "    train2 = data2[:X.shape[0]]\n",
    "    test2 = data2[-X_test.shape[0]:]\n",
    "\n",
    "    train2 = pd.DataFrame(train2, columns=[f'pca_c-{i}' for i in range(data2.shape[1])])\n",
    "    test2 = pd.DataFrame(test2, columns=[f'pca_c-{i}' for i in range(data2.shape[1])])\n",
    "\n",
    "    X = pd.concat((X, train2), axis=1)\n",
    "    X_test = pd.concat((X_test, test2), axis=1)\n",
    "\n",
    "    clusters = params.get('n_clusters', 0)\n",
    "    if clusters:\n",
    "        X, X_test = fe_cluster_pca(X, X_test, n_clusters=clusters, seed=random_state)\n",
    "    return X, X_test\n",
    "\n",
    "gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']\n",
    "\n",
    "def fe_stats(train, test, **params):\n",
    "\n",
    "    if not params.get('enabled', True):\n",
    "        return train, test\n",
    "\n",
    "    features_g, features_c = g_c_features(train)\n",
    "    for df in train, test:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1)\n",
    "        df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1)\n",
    "        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1)\n",
    "        df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1)\n",
    "        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n",
    "        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "    \n",
    "        df['c52_c42'] = df['c-52'] * df['c-42']\n",
    "        df['c13_c73'] = df['c-13'] * df['c-73']\n",
    "        df['c26_c13'] = df['c-23'] * df['c-13']\n",
    "        df['c33_c6'] = df['c-33'] * df['c-6']\n",
    "        df['c11_c55'] = df['c-11'] * df['c-55']\n",
    "        df['c38_c63'] = df['c-38'] * df['c-63']\n",
    "        df['c38_c94'] = df['c-38'] * df['c-94']\n",
    "        df['c13_c94'] = df['c-13'] * df['c-94']\n",
    "        df['c4_c52'] = df['c-4'] * df['c-52']\n",
    "        df['c4_c42'] = df['c-4'] * df['c-42']\n",
    "        df['c13_c38'] = df['c-13'] * df['c-38']\n",
    "        df['c55_c2'] = df['c-55'] * df['c-2']\n",
    "        df['c55_c4'] = df['c-55'] * df['c-4']\n",
    "        df['c4_c13'] = df['c-4'] * df['c-13']\n",
    "        df['c82_c42'] = df['c-82'] * df['c-42']\n",
    "        df['c66_c42'] = df['c-66'] * df['c-42']\n",
    "        df['c6_c38'] = df['c-6'] * df['c-38']\n",
    "        df['c2_c13'] = df['c-2'] * df['c-13']\n",
    "        df['c62_c42'] = df['c-62'] * df['c-42']\n",
    "        df['c90_c55'] = df['c-90'] * df['c-55']\n",
    "        for feature in features_c:\n",
    "             df[f'{feature}_squared'] = df[feature] ** 2     \n",
    "\n",
    "        for feature in gsquarecols:\n",
    "            df[f'{feature}_squared'] = df[feature] ** 2\n",
    "    return train, test\n",
    "\n",
    "def preprocess_X(params, X, X_test, y, y0, seed=42):\n",
    "    p = params\n",
    "    p_scaler = p['scaler']\n",
    "    p_pca = p['pca']\n",
    "    p_fe_cluster = p['fe_cluster']\n",
    "    p_fe_stats = p['fe_stats']\n",
    "    p_variance_reduction = p['variance_reduction']\n",
    "#     print(X.shape, 'initial')\n",
    "    if p_scaler == 'quantile':\n",
    "        X, X_test = quantile_transformer(X, X_test, seed=seed)\n",
    "    elif p_scaler == 'gauss':\n",
    "        X = rank_gauss(X)\n",
    "        X_test = rank_gauss(X_test)\n",
    "    elif p_scaler == 'standard':\n",
    "        X, X_test = standard_scaler(X, X_test)\n",
    "    elif p_scaler != 'none':\n",
    "        raise Exception(f'Unknown scaler: {p_scaler}')\n",
    "#     print(X.shape, 'scaler')\n",
    "    X, X_test = pca_transformer(X, X_test, seed=seed, **p_pca)\n",
    "#     print(X.shape, 'pca')\n",
    "    X, X_test = fe_cluster(X, X_test, seed=seed, **p_fe_cluster)\n",
    "#     print(X.shape, 'cluster')\n",
    "    X, X_test = fe_stats(X, X_test, **p_fe_stats)\n",
    "#     print(X.shape, 'fe_stats')\n",
    "    X, X_test = variance_reduction(X, X_test, **p_variance_reduction)\n",
    "#     print(X.shape, 'variance')\n",
    "    if p.get(\"shuffle_cols\", True):\n",
    "        X, X_test = shuffle_cols(X, X_test)\n",
    "\n",
    "    y0 = y0[X['cp_type'] == 0].reset_index(drop = True)\n",
    "    y = y[X['cp_type'] == 0].reset_index(drop = True)\n",
    "    X = X[X['cp_type'] == 0].reset_index(drop = True)\n",
    "\n",
    "    X = X.drop(['sig_id'], axis=1, errors='ignore')\n",
    "    X_test = X_test.drop(['sig_id'], axis=1, errors='ignore')\n",
    "    return X, X_test, y, y0\n",
    "\n",
    "def shuffle_cols(train, test):\n",
    "    # В зависимости от seed перемешиваем фичи\n",
    "    features_shrink = 1.\n",
    "    inp_size = int(np.ceil(features_shrink * len(train.columns)))\n",
    "    split_cols = np.random.choice(train.columns, inp_size, replace=False)\n",
    "    return train[split_cols], test[split_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:28:10.466276Z",
     "start_time": "2020-11-29T17:28:10.426769Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_test_split(X_f, y_f, n_split=7, seed=42):\n",
    "    return fold(X_f, y_f, n_split, seed)[0]\n",
    "\n",
    "def fold_simple(X_f, y_f, n_split, seed):\n",
    "    if len(y_f.columns) > 1:\n",
    "        f = MultilabelStratifiedKFold(n_splits = n_split, random_state = seed, shuffle = True)\n",
    "        return list(f.split(X_f, y_f))\n",
    "    else:\n",
    "        f = StratifiedKFold(n_splits = n_split, random_state=seed, shuffle = True)\n",
    "        return list(f.split(X_f, y_f))\n",
    "\n",
    "def fold_drug(X_f, y_f, drug_thresh=18, n_split=7, seed=42):\n",
    "    vc = y_f.drug_id.value_counts()\n",
    "    vc1 = vc.loc[vc <= drug_thresh].index.sort_values()\n",
    "    vc2 = vc.loc[vc > drug_thresh].index.sort_values()\n",
    "    \n",
    "    target_cols = [c for c in y_f.columns if c != 'drug_id']\n",
    "\n",
    "    # Сначала бьём на фолды лекарства\n",
    "    # STRATIFY DRUGS 18X OR LESS\n",
    "    skf1 = MultilabelStratifiedKFold(n_splits=n_split, shuffle=True, random_state=seed)\n",
    "    tmp1 = y_f.groupby('drug_id').mean().loc[vc1]\n",
    "    split_1 = list(skf1.split(tmp1, tmp1[target_cols]))\n",
    "\n",
    "    # STRATIFY DRUGS MORE THAN 18X\n",
    "    skf2 = MultilabelStratifiedKFold(n_splits=n_split, shuffle=True, random_state=seed)\n",
    "    tmp2 = y_f.loc[y_f.drug_id.isin(vc2)].reset_index()\n",
    "    split_2 = list(skf2.split(tmp2[target_cols], tmp2[target_cols]))\n",
    "\n",
    "    folds = []\n",
    "    for i in range(n_split):\n",
    "        ind_tr_drug, ind_val_drug = split_1[i]\n",
    "        tr_drug, val_drug = tmp1.iloc[ind_tr_drug].index, tmp1.iloc[ind_val_drug].index\n",
    "        ind_tr_1, ind_val_1 = y_f.loc[y_f.drug_id.isin(tr_drug)].index, y_f.loc[y_f.drug_id.isin(val_drug)].index\n",
    "        \n",
    "        ind_tr_2, ind_val_2 = split_2[i]\n",
    "        ind_tr_2, ind_val_2 = tmp2.iloc[ind_tr_2]['index'], tmp2.iloc[ind_val_2]['index']\n",
    "\n",
    "        ind_tr = np.concatenate([ind_tr_1, ind_tr_2])\n",
    "        ind_val = np.concatenate([ind_val_1, ind_val_2])\n",
    "        folds.append((ind_tr, ind_val))\n",
    "    return folds\n",
    "\n",
    "fold = fold_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:29:31.378064Z",
     "start_time": "2020-11-29T17:29:31.359074Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ParamScheduler:\n",
    "    def __init__(self, start, end, num_iter):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.num_iter = num_iter\n",
    "        self.idx = -1\n",
    "        \n",
    "    def func(self, start_val, end_val, pct):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def step(self):\n",
    "        self.idx+=1\n",
    "        return self.func(self.start, self.end, self.idx/self.num_iter)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.idx=-1\n",
    "        \n",
    "    def is_complete(self):\n",
    "        return self.idx >= self.num_iter\n",
    "\n",
    "class LinearScheduler(ParamScheduler):\n",
    "    \n",
    "    def func(self, start_val, end_val, pct):\n",
    "        return start_val + pct * (end_val - start_val)\n",
    "    \n",
    "class CosineScheduler(ParamScheduler):\n",
    "    \n",
    "    def func(self, start_val, end_val, pct):\n",
    "        cos_out = np.cos(np.pi * pct) + 1\n",
    "        return end_val + (start_val - end_val)/2 * cos_out\n",
    "\n",
    "class OneCycleScheduler(Callback):\n",
    "    \n",
    "    def __init__(self, max_lr, momentums=(0.95,0.85), start_div=25., pct_start=0.3, verbose=True, sched=CosineScheduler, end_div=None):\n",
    "        self.max_lr, self.momentums, self.start_div, self.pct_start, self.verbose, self.sched, self.end_div = max_lr, momentums, start_div, pct_start, verbose, sched, end_div\n",
    "        if self.end_div is None:\n",
    "            self.end_div = start_div * 1e4\n",
    "        self.logs = {}\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.num_epochs = self.params['epochs']\n",
    "        self.steps_per_epoch = self.params['steps']\n",
    "        start_lr = self.max_lr/self.start_div\n",
    "        end_lr = self.max_lr/self.end_div\n",
    "        num_iter = self.num_epochs * self.steps_per_epoch\n",
    "        num_iter_1 = int(self.pct_start*num_iter)\n",
    "        num_iter_2 = num_iter - num_iter_1\n",
    "        self.lr_scheds = (self.sched(start_lr, self.max_lr, num_iter_1), self.sched(self.max_lr, end_lr, num_iter_2))\n",
    "        self.momentum_scheds = (self.sched(self.momentums[0], self.momentums[1], num_iter_1), self.sched(self.momentums[1], self.momentums[0], num_iter_2))\n",
    "        self.sched_idx = 0\n",
    "        self.optimizer_params_step()   \n",
    "        \n",
    "    def optimizer_params_step(self):\n",
    "        next_lr = self.lr_scheds[self.sched_idx].step()\n",
    "        next_momentum = self.momentum_scheds[self.sched_idx].step()\n",
    "        \n",
    "        # add to logs\n",
    "        self.logs.setdefault('lr', []).append(next_lr)\n",
    "        self.logs.setdefault('momentum', []).append(next_momentum)\n",
    "        \n",
    "        # update optimizer params\n",
    "        K.set_value(self.model.optimizer.lr, next_lr)\n",
    "        if hasattr(self.model.optimizer, 'momentum'):\n",
    "            K.set_value(self.model.optimizer.momentum, next_momentum)\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.sched_idx >= len(self.lr_scheds):\n",
    "            self.model.stop_training=True\n",
    "            return\n",
    "        self.optimizer_params_step()\n",
    "        if self.lr_scheds[self.sched_idx].is_complete():\n",
    "            self.sched_idx += 1\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.verbose:\n",
    "            if hasattr(self.model.optimizer, 'momentum'):\n",
    "                print(\"- OneCycleScheduler, lr: {}, momentum: {}\".format(self.logs['lr'][-1], self.logs['momentum'][-1]))\n",
    "            else:\n",
    "                print(\"- OneCycleScheduler, lr: {}\".format(self.logs['lr'][-1]))\n",
    "            \n",
    "        if epoch >= self.num_epochs:\n",
    "            self.model.stop_training=True\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:29:38.912555Z",
     "start_time": "2020-11-29T17:29:38.896606Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "p_min = 1e-10\n",
    "p_max = 1-1e-10\n",
    "\n",
    "def y_to_zero_class(df_y):\n",
    "    return pd.DataFrame(df_y.max(axis=1).map({1: 0, 0: 1}))\n",
    "\n",
    "def log_loss_metric(y_true, y_pred, columns=None):\n",
    "    metrics = []\n",
    "    y_pred = np.clip(y_pred, p_min, p_max)\n",
    "    cols = y_true.columns if columns is None else columns\n",
    "    for _target in cols:\n",
    "        metrics.append(\n",
    "            log_loss(\n",
    "                y_true.loc[:, _target],\n",
    "                y_pred.loc[:, _target].astype(float),\n",
    "                labels = [0, 1]\n",
    "            )\n",
    "        )\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def log_loss_result(y_true, y_pred):\n",
    "    return log_loss_metric(y_true, y_pred, columns=y.columns)\n",
    "\n",
    "def logloss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n",
    "    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n",
    "\n",
    "checkpoint_path = \"model.h5\"\n",
    "\n",
    "def callbacks(verbose=0):\n",
    "    rlr = ReduceLROnPlateau(\n",
    "        monitor = 'val_logloss', factor = 0.1, patience = 3, verbose = verbose, \n",
    "        min_delta=1e-4, mode = 'min'\n",
    "    )\n",
    "#     ckp = ModelCheckpoint(\n",
    "#         checkpoint_path, monitor = 'val_logloss', verbose = 0, \n",
    "#         save_best_only = True, mode = 'min'\n",
    "#     )\n",
    "    lr_scheduler = OneCycleScheduler(\n",
    "        1e-2, start_div=1e3, pct_start=0.1, verbose=True, end_div=10000.0\n",
    "    )\n",
    "    es = EarlyStopping(\n",
    "        monitor = 'val_logloss', min_delta = 1e-5, patience = 10, mode = 'min', \n",
    "        baseline = None, restore_best_weights = True, verbose = verbose\n",
    "    )\n",
    "    return rlr, es#, ckp\n",
    "\n",
    "def y_arr_to_df(y_arr, cols=None):\n",
    "    if cols is None:\n",
    "        cols = y.columns\n",
    "    return pd.DataFrame(y_arr, columns=cols)\n",
    "\n",
    "error_classes = [\n",
    "    'cyclooxygenase_inhibitor',\n",
    "    'dopamine_receptor_antagonist',\n",
    "    'glutamate_receptor_antagonist',\n",
    "    'adrenergic_receptor_antagonist',\n",
    "    'dna_inhibitor'\n",
    "]\n",
    "def y_to_error_classes(y_df):\n",
    "#     y_df_wo = pd.DataFrame(y_df.drop(error_classes, axis=1).max(axis=1))\n",
    "#     y_df_wo.columns = ['other']\n",
    "    y_df_w = y_df[error_classes]\n",
    "#     y_res = pd.concat([y_df_wo, y_df_w], axis=1)\n",
    "    return y_df_w\n",
    "\n",
    "y_to_error_classes(y).sum(axis=0)\n",
    "\n",
    "EPOCHS = None\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep multilabel model keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:53:27.912467Z",
     "start_time": "2020-11-29T17:53:27.890430Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import gc\n",
    "\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    gc.collect() # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "#     config = tf.ConfigProto()\n",
    "#     config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "#     config.gpu_options.visible_device_list = \"0\"\n",
    "#     K.set_session(tf.Session(config=config))\n",
    "\n",
    "reset_keras()\n",
    "\n",
    "def create_model(\n",
    "    num_cols_x, num_cols_y, hid_layers, \n",
    "    activations, dropout_rate, \n",
    "    lr, label_smoothing, \n",
    "    weight_decay=1e-5,\n",
    "    batch_norm=True, weight_norm=True\n",
    "):\n",
    "    inp1 = tf.keras.layers.Input(shape = (num_cols_x, ))\n",
    "    x1 = inp1\n",
    "    if batch_norm:\n",
    "        x1 = tf.keras.layers.BatchNormalization(epsilon=1e-05, momentum=0.9)(x1)\n",
    "#     x1 = tf.keras.layers.Dropout(dropout_rate[0])(x1)\n",
    "\n",
    "    for i, units in enumerate(hid_layers):\n",
    "        activation = activations[i]\n",
    "\n",
    "        if activation == 'leaky_relu':\n",
    "            dense = tf.keras.layers.Dense(units)\n",
    "        else:\n",
    "            dense = tf.keras.layers.Dense(units, activation=activation)\n",
    "\n",
    "        if weight_norm and weight_norm != 'output':\n",
    "            x1 = WeightNormalization(dense)(x1)\n",
    "        else:\n",
    "            x1 = dense(x1)\n",
    "\n",
    "        if activation == 'leaky_relu':\n",
    "            x1 = tf.keras.layers.LeakyReLU(alpha=0.01)(x1)\n",
    "\n",
    "        if batch_norm:\n",
    "            x1 = tf.keras.layers.BatchNormalization(epsilon=1e-05, momentum=0.9)(x1)\n",
    "        x1 = tf.keras.layers.Dropout(dropout_rate[i])(x1)\n",
    "    \n",
    "    out_dense = tf.keras.layers.Dense(num_cols_y, activation='sigmoid')\n",
    "    if weight_norm:\n",
    "        x1 = WeightNormalization(out_dense)(x1)\n",
    "    else:\n",
    "        x1 = out_dense(x1)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inp1, outputs=x1)    \n",
    "    opt = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=weight_decay)\n",
    "#     opt = tfa.optimizers.Lookahead(opt, sync_period=10)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing), \n",
    "        metrics=logloss\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class DeepMultiLabelModelKeras(object):\n",
    "    def __init__(self, params, model_name, verbose=0, seed=42):\n",
    "        self.deep_params = params\n",
    "        self.model_name = model_name\n",
    "        self.seed = seed\n",
    "        self.num_epochs = EPOCHS or params['epochs']\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.verbose = verbose\n",
    "        self.classes = []\n",
    "\n",
    "    @property\n",
    "    def definition(self):\n",
    "        model_info = []\n",
    "        if self.model:\n",
    "            self.model.summary(print_fn=model_info.append)\n",
    "        return \"\\n\".join(model_info)\n",
    "    \n",
    "    def fit(self, x_tr, y_tr, x_val, y_val, y0_tr, y0_val, foldno=None):\n",
    "        inp_size = x_tr.shape[1]\n",
    "        y_shape = y_val.shape[1]\n",
    "        hid_layer = self.deep_params['hid_layer']\n",
    "        activation = self.deep_params['activation']\n",
    "        dropout = self.deep_params['dropout']\n",
    "        learning_rate = self.deep_params['learning_rate']\n",
    "        label_smoothing = self.deep_params['label_smoothing']\n",
    "        batch_norm = self.deep_params.get('batch_norm', True)\n",
    "        weight_norm = self.deep_params.get('weight_norm', True)\n",
    "        init_non_scored_weights = self.deep_params.get('init_non_scored_weights', True)\n",
    "        cls_weight = self.deep_params.get('class_weight')\n",
    "        weight_decay = self.deep_params.get('weight_decay', 1e-5)\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        # Scored модель\n",
    "        model = create_model(\n",
    "            inp_size, y_shape, hid_layer, activation, dropout, \n",
    "            learning_rate, label_smoothing,\n",
    "            weight_decay=weight_decay,\n",
    "            batch_norm=batch_norm, weight_norm=weight_norm\n",
    "        )\n",
    "        \n",
    "        if init_non_scored_weights:\n",
    "            ns_y_tr = y0_tr\n",
    "            ns_y_val = y0_val\n",
    "            if init_non_scored_weights == 'ALL_TARGETS':\n",
    "                ns_y_tr = np.hstack([y_tr, y0_tr])\n",
    "                ns_y_val = np.hstack([y_val, y0_val])\n",
    "\n",
    "            # Non-scored модель\n",
    "            model0 = create_model(\n",
    "                inp_size, ns_y_val.shape[1], hid_layer, activation, dropout, \n",
    "                learning_rate, label_smoothing,\n",
    "                weight_decay=weight_decay,\n",
    "                batch_norm=batch_norm, weight_norm=weight_norm\n",
    "            )\n",
    "            model0.fit(\n",
    "                x_tr, ns_y_tr, validation_data=(x_val, ns_y_val),\n",
    "                epochs = self.num_epochs, batch_size = batch_size,\n",
    "                callbacks = callbacks(self.verbose), verbose = self.verbose\n",
    "            )\n",
    "            # Transfer weights\n",
    "            for i in range(len(model.layers)-1):\n",
    "                model.layers[i].set_weights(model0.layers[i].get_weights())\n",
    "\n",
    "        self.history = model.fit(\n",
    "            x_tr, y_tr, validation_data=(x_val, y_val),\n",
    "            epochs = self.num_epochs, batch_size = batch_size,\n",
    "            callbacks = callbacks(self.verbose), verbose = self.verbose,\n",
    "            class_weight = cls_weight\n",
    "        )\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.astype('float64').values\n",
    "        return self.model.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep multilabel model torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:29:42.932193Z",
     "start_time": "2020-11-29T17:29:42.908828Z"
    }
   },
   "outputs": [],
   "source": [
    "class FineTuneScheduler:\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def step(self, epoch, model):\n",
    "        if len(model.frozen_layers) == 0:\n",
    "            return\n",
    "\n",
    "        epochs_per_step = self.epochs // len(model.frozen_layers)\n",
    "\n",
    "        if epoch % epochs_per_step == 0:\n",
    "            last_frozen_index = model.frozen_layers[-1]\n",
    "            \n",
    "            # Unfreeze parameters of the last frozen layer\n",
    "            for name, param in model.named_parameters():\n",
    "                layer_index = name.split('.')[0][-1]\n",
    "\n",
    "                if layer_index == last_frozen_index:\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            del model.frozen_layers[-1]  # Remove the last layer as unfrozen\n",
    "\n",
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct  \n",
    "\n",
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight)\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:51:10.310924Z",
     "start_time": "2020-11-29T17:51:10.278551Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MOAModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, deep_params, num_features, num_targets, state_path):\n",
    "        super(MOAModel, self).__init__()\n",
    "        self.deep_params = deep_params\n",
    "        self.state_path = state_path\n",
    "        self.num_features = num_features\n",
    "        self.num_targets = num_targets\n",
    "\n",
    "        self.hidden_size = deep_params['hid_layer']\n",
    "        activations = deep_params['activation']\n",
    "        dropout = deep_params['dropout']\n",
    "        batch_norm = deep_params.get('batch_norm', True)\n",
    "        weight_norm = deep_params.get('weight_norm', True)\n",
    "\n",
    "        layers = []\n",
    "        if batch_norm:\n",
    "            self.batch_norm0 = nn.BatchNorm1d(num_features)\n",
    "            layers.append(self.batch_norm0)\n",
    "        \n",
    "        prev_size = num_features\n",
    "        for i, hidden_size in enumerate(self.hidden_size):\n",
    "            layer_index = i + 1\n",
    "            activation = activations[i]\n",
    "\n",
    "            dense = nn.Linear(prev_size, hidden_size)\n",
    "            if weight_norm and weight_norm != 'output':\n",
    "                layer = nn.utils.weight_norm(dense)\n",
    "                self.add_module(f'dense_weight{layer_index}', layer)\n",
    "                layers.append([layer, self.recalibrate_layer])\n",
    "            else:\n",
    "                self.add_module(f'dense{layer_index}', dense)\n",
    "                layers.append(dense)\n",
    "            \n",
    "            if activation == 'leaky_relu':\n",
    "                layers.append(F.leaky_relu)\n",
    "            elif activation == 'relu':\n",
    "                layers.append(F.relu)\n",
    "            elif activation == 'elu':\n",
    "                layers.append(F.elu)\n",
    "            elif activation == 'selu':\n",
    "                layers.append(F.selu)\n",
    "\n",
    "            if batch_norm:\n",
    "                layer = nn.BatchNorm1d(hidden_size)\n",
    "                self.add_module(f'batch_norm{layer_index}', layer)\n",
    "                layers.append(layer)\n",
    "            layer = nn.Dropout(dropout[i])\n",
    "            self.add_module(f'dropout{layer_index}', layer)\n",
    "            layers.append(layer)\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        self.hidden_layers = layers\n",
    "        self.setup_output(prev_size, num_targets)\n",
    "    \n",
    "    def setup_output(self, prev_size, num_targets):\n",
    "        weight_norm = self.deep_params.get('weight_norm', True)\n",
    "        out_dense = nn.Linear(prev_size, num_targets)\n",
    "        if weight_norm:\n",
    "            self.dense_weight9 = nn.utils.weight_norm(out_dense)\n",
    "            self.layers = self.hidden_layers + [[self.dense_weight9, self.recalibrate_layer]]\n",
    "        else:\n",
    "            self.dense9 = out_dense\n",
    "            self.layers = self.hidden_layers + [self.dense9]\n",
    "    \n",
    "    def recalibrate_layer(self, layer):\n",
    "        if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "        if(torch.isnan(layer.weight).sum() > 0):\n",
    "            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "            layer.weight += 1e-7\n",
    "        return layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, list):\n",
    "                x = layer[1](layer[0])(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def transfer_weights(self, model):\n",
    "        prev_size = self.hidden_size[-1]\n",
    "        self.frozen_layers = []\n",
    "        self.setup_output(prev_size, model.num_targets)\n",
    "        self.load_state_dict(model.state_dict())\n",
    "        self.to(DEVICE)\n",
    "\n",
    "        # Freeze all weights\n",
    "        for name, param in self.named_parameters():\n",
    "            layer_index = name.split('.')[0][-1]\n",
    "            if layer_index == '9':\n",
    "                continue\n",
    "            param.requires_grad = False\n",
    "            # Save frozen layer names\n",
    "            if layer_index not in self.frozen_layers:\n",
    "                self.frozen_layers.append(layer_index)\n",
    "        \n",
    "        self.setup_output(prev_size, self.num_targets)\n",
    "        self.to(DEVICE)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        testdataset = TestDataset(x_test)\n",
    "        testloader = torch.utils.data.DataLoader(testdataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.load_state_dict(torch.load(self.state_path))\n",
    "        self.to(DEVICE)\n",
    "\n",
    "        return inference_fn(self, testloader, DEVICE)\n",
    "\n",
    "    def fit(self, x_train, y_train, validation_data, verbose=0, foldno=None, fine_tune_scheduler=None):\n",
    "        x_valid, y_valid = validation_data\n",
    "        model = self\n",
    "        self.to(DEVICE)\n",
    "\n",
    "        learning_rate = self.deep_params['learning_rate']\n",
    "        label_smoothing = self.deep_params['label_smoothing']\n",
    "        weight_decay = self.deep_params.get('weight_decay', 1e-5)\n",
    "        num_epochs = EPOCHS or self.deep_params['epochs']\n",
    "        early_stopping_steps = self.deep_params.get('early_stopping_steps') or 0\n",
    "\n",
    "        train_dataset = MoADataset(x_train, y_train)\n",
    "        valid_dataset = MoADataset(x_valid, y_valid)\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "            max_lr=1e-2, epochs=num_epochs, steps_per_epoch=len(trainloader)\n",
    "        )\n",
    "\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        loss_tr = SmoothBCEwLogits(smoothing=label_smoothing)\n",
    "        \n",
    "        best_loss = np.inf\n",
    "        early_step = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            if fine_tune_scheduler is not None:\n",
    "                fine_tune_scheduler.step(epoch, model)\n",
    "\n",
    "            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n",
    "            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "            if verbose or epoch % 5 == 0:\n",
    "                print(f\"FOLD: {foldno}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                torch.save(model.state_dict(), self.state_path)\n",
    "\n",
    "            elif early_stopping_steps:\n",
    "                early_step += 1\n",
    "                if (early_step >= early_stopping_steps):\n",
    "                    break\n",
    "\n",
    "!mkdir -p torch\n",
    "class DeepMultiLabelModelTorch(object):\n",
    "    def __init__(self, params, model_name, verbose=0, seed=42):\n",
    "        self.deep_params = params\n",
    "        self.model_name = model_name\n",
    "        self.seed = seed\n",
    "        self.num_epochs = EPOCHS or params['epochs']\n",
    "        self.model = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @property\n",
    "    def definition(self):\n",
    "        return json.dumps(self.deep_params)\n",
    "    \n",
    "    def fit(self, x_tr, y_tr, x_val, y_val, y0_tr, y0_val, foldno):\n",
    "        inp_size = x_tr.shape[1]\n",
    "        y_shape = y_val.shape[1]\n",
    "        deep_params = self.deep_params\n",
    "        init_non_scored_weights = deep_params.get('init_non_scored_weights', True)\n",
    "\n",
    "        state_path = os.path.join(f\"torch/{self.model_name}_SEED_{self.seed}_FOLD{foldno}.pth\")\n",
    "        model = MOAModel(self.deep_params, inp_size, y_shape, state_path)\n",
    "\n",
    "        fine_tune_scheduler = None\n",
    "\n",
    "        if init_non_scored_weights:\n",
    "            ns_y_tr = y0_tr\n",
    "            ns_y_val = y0_val\n",
    "            fine_tune_scheduler = FineTuneScheduler(self.num_epochs)\n",
    "            if init_non_scored_weights == 'ALL_TARGETS':\n",
    "                ns_y_tr = np.hstack([y_tr, y0_tr])\n",
    "                ns_y_val = np.hstack([y_val, y0_val])\n",
    "            \n",
    "            # Train on scored + nonscored targets\n",
    "            state_path0 = os.path.join(f\"torch/{self.model_name}_ns_SEED_{self.seed}_FOLD{foldno}.pth\")\n",
    "            model0 = MOAModel(deep_params, inp_size, ns_y_val.shape[1], state_path0)\n",
    "            model0.fit(x_tr, ns_y_tr, validation_data=(x_val, ns_y_val), verbose=self.verbose, foldno=foldno)\n",
    "            model.transfer_weights(model0)\n",
    "\n",
    "        model.fit(\n",
    "            x_tr, y_tr, validation_data=(x_val, y_val),\n",
    "            verbose=self.verbose, foldno=foldno, fine_tune_scheduler=fine_tune_scheduler\n",
    "        )\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.astype('float64').values\n",
    "        return self.model.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep multilabel CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:51:12.543319Z",
     "start_time": "2020-11-29T17:51:12.502369Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeepMultiLabelModelCV(object):\n",
    "    def __init__(self, params, model_name, verbose=0, seed=42, base_model=DeepMultiLabelModelTorch):\n",
    "        self.deep_params = params\n",
    "        self.model_name = model_name\n",
    "        self.seed = seed\n",
    "        self.num_epochs = EPOCHS or params['epochs']\n",
    "        self.verbose = verbose\n",
    "        self.cv_models = []\n",
    "        self.classes = []\n",
    "        self.base_model = base_model\n",
    "    \n",
    "    @property\n",
    "    def real_model(self):\n",
    "        if self.cv_models:\n",
    "            return self.cv_models[0]\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def definition(self):\n",
    "        if not self.cv_models:\n",
    "            return ''\n",
    "        return \"\\n\".join(self.cv_models[0].definition)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.astype('float64').values\n",
    "        preds = []\n",
    "        for model in self.cv_models:\n",
    "            preds.append(model.predict(X))\n",
    "        return np.mean(preds, axis=0)\n",
    "        \n",
    "    def cv(self, X, y, y0, run_name=None, n_split=7, return_pred=False,\n",
    "           metric_fn=log_loss_metric, overfit=True, run_tags=None, max_score=None):\n",
    "        seed = self.seed\n",
    "        self.cv_models = []\n",
    "        splits = fold(X, y, n_split=n_split, seed=self.seed)\n",
    "        \n",
    "        y = y.drop(columns=['drug_id'], errors='ignore')\n",
    "        y0 = y0.drop(columns=['drug_id'], errors='ignore')\n",
    "\n",
    "        ycols = y.columns\n",
    "        self.classes = ycols\n",
    "        yvals = y.astype(float).values\n",
    "        y0vals = y0.astype(float).values\n",
    "        X_vals = X.astype(float).values\n",
    "\n",
    "        test_pred = pd.DataFrame(index=X.index, columns=ycols)\n",
    "        test_pred.loc[:,:] = 0\n",
    "        model_def = None\n",
    "        \n",
    "        initial_time = time()\n",
    "        for n, (tr, te) in enumerate(splits):\n",
    "            start_time = time()\n",
    "            # Обучающая,Валидационная выборка\n",
    "            x_tr, x_test = X_vals[tr], X_vals[te]\n",
    "            # Y предварительной задачи (non-scored)\n",
    "            y0_tr, y0_test = y0vals[tr], y0vals[te]\n",
    "            # Y основной задачи\n",
    "            y_tr, y_test = yvals[tr], yvals[te]\n",
    "\n",
    "            # Разбиваем обучение еще на train/val\n",
    "            if not overfit:\n",
    "                ind_tr, ind_val = train_test_split(pd.DataFrame(x_tr), pd.DataFrame(y_tr))\n",
    "                x_tr, x_val = x_tr[ind_tr], x_tr[ind_val]\n",
    "                y_tr, y_val = y_tr[ind_tr], y_tr[ind_val]\n",
    "                y0_tr, y0_val = y0_tr[ind_tr], y0_tr[ind_val]\n",
    "            else:\n",
    "                x_val = x_test\n",
    "                y_val = y_test\n",
    "                y0_val = y0_test\n",
    "\n",
    "            model = self.base_model(\n",
    "                self.deep_params, model_name=self.model_name, \n",
    "                verbose=self.verbose, seed=self.seed\n",
    "            )\n",
    "            model.fit(x_tr, y_tr, x_val, y_val, y0_tr, y0_val, foldno=n)\n",
    "            model_def = model.definition\n",
    "\n",
    "            test_pred.loc[te, ycols] = model.predict(x_test)\n",
    "\n",
    "            self.cv_models.append(model)\n",
    "\n",
    "            oof = metric_fn(y.loc[te, ycols], test_pred.loc[te, ycols])\n",
    "            print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}], Fold {n}: {oof}')\n",
    "            if max_score and oof > max_score:\n",
    "                print(f'break cv execution {oof} > {max_score}')\n",
    "                if not return_pred:\n",
    "                    return None\n",
    "                else:\n",
    "                    return None, None\n",
    "            reset_keras()\n",
    "\n",
    "        logloss_valid = metric_fn(y, test_pred)\n",
    "        print(f'Valid logloss: {logloss_valid}')\n",
    "        if has_internet and run_name:\n",
    "            mlflow.set_experiment('Kaggle-MOA-{}'.format(self.model_name))\n",
    "            with mlflow.start_run(run_name=run_name):\n",
    "                mlflow.log_params(dict_flatten({\n",
    "                    'n_split': n_split,\n",
    "                    'p_min': p_min,\n",
    "                    'p_max': p_max,\n",
    "                    'nn': self.deep_params\n",
    "                }))\n",
    "                mlflow.log_metric(key=\"logloss_valid\", value=logloss_valid)\n",
    "                run_tags = run_tags or {}\n",
    "                run_tags.update({\n",
    "                    'model_def': model_def,\n",
    "                    'run': run_name,\n",
    "                    'run_time': time() - initial_time\n",
    "                })\n",
    "                mlflow.set_tags(run_tags)\n",
    "        if not return_pred:\n",
    "            return logloss_valid\n",
    "        else:\n",
    "            return logloss_valid, test_pred\n",
    "    \n",
    "    def errors(self, X, y):\n",
    "        # Количество ошибок, когда должно быть везде 0, а на самом деле - нет\n",
    "        # Количество ошибок, когда должно что-то быть, а на самом деле везде 0\n",
    "        # Для уменьшения этих ошибок можно применять lgb_zero\n",
    "        y = y.drop(columns=['drug_id'], errors='ignore')\n",
    "        ycols = y.columns\n",
    "        seed_everything(self.seed)\n",
    "\n",
    "        _, te = train_test_split(X, y, n_split=5)\n",
    "\n",
    "        x_val = X_p.astype('float64').values[te]\n",
    "\n",
    "        y_true = y.loc[te, ycols].reset_index(drop=True)\n",
    "        y_pred = y_arr_to_df(self.predict(x_val), ycols)\n",
    "\n",
    "        non_zeros = y_true[(y_true.T != 0).any()].index\n",
    "        all_zeros = y_true[~(y_true.T != 0).any()].index\n",
    "\n",
    "        clip_p_min = 1e-5\n",
    "        clip_p_max = 1 - 1e-5\n",
    "\n",
    "        y_pred_clip = np.clip(y_pred, clip_p_min, clip_p_max)\n",
    "\n",
    "        print('Logloss:', log_loss_metric(y_true, y_pred))\n",
    "        print('Logloss all zeros:', log_loss_metric(y_true.loc[all_zeros, :], y_pred.loc[all_zeros, :]))\n",
    "        print('Logloss non zeros:', log_loss_metric(y_true.loc[non_zeros, :], y_pred.loc[non_zeros, :]))\n",
    "        print('Logloss clip:', log_loss_metric(y_true, y_pred_clip))\n",
    "        print('Logloss all zeros clip:', log_loss_metric(y_true.loc[all_zeros, :], y_pred_clip.loc[all_zeros, :]))\n",
    "        print('Logloss non zeros clip:', log_loss_metric(y_true.loc[non_zeros, :], y_pred_clip.loc[non_zeros, :]))\n",
    "\n",
    "        losses = []\n",
    "        for i in range(y_true.shape[1]):\n",
    "            y_true_cl = y_true.iloc[:,i]\n",
    "            y_pred_cl = y_pred.iloc[:,i]\n",
    "            losses.append({\n",
    "                \"index\": i,\n",
    "                \"class\": y_true_cl.name,\n",
    "                'true_0': len(y_true_cl[y_true_cl == 0]),\n",
    "                'true_1': len(y_true_cl[y_true_cl == 1]),\n",
    "                \"loss\": log_loss(y_true_cl.values, y_pred_cl.values, labels=[0, 1]),\n",
    "                'pred_hist_0': y_pred_cl[y_pred_cl <= 0.5].round(1).value_counts().sort_index().reset_index().values,\n",
    "                'pred_hist_1': y_pred_cl[y_pred_cl > 0.5].round(1).value_counts().sort_index().reset_index().values,\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(losses).set_index(['index', 'class']).sort_values('loss', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:30:38.346905Z",
     "start_time": "2020-11-29T17:30:38.334519Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_to_terms(s):\n",
    "    return s.split('_')\n",
    "\n",
    "terms = pd.DataFrame({'terms': y.columns.map(split_to_terms)}).explode('terms')['terms'].value_counts()\n",
    "terms = list(terms[terms > 1].index)\n",
    "terms_map = {t: i for i, t in enumerate(terms)}\n",
    "\n",
    "def y_to_terms(y_df):\n",
    "    term_vals = []\n",
    "    for _, row in y_df.iterrows():\n",
    "        new_classes = [0] * len(terms)\n",
    "        terms_1 = set(pd.DataFrame(row[row > 0].index.map(split_to_terms)).explode(0)[0].values.tolist())\n",
    "        for term in terms_1:\n",
    "            if term not in terms_map:\n",
    "                continue\n",
    "            new_classes[terms_map[term]] = 1\n",
    "        term_vals.append(new_classes)\n",
    "    return pd.DataFrame(term_vals, columns=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:30:38.932364Z",
     "start_time": "2020-11-29T17:30:38.930024Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess_params, deep_params, _, _, _ = ({'fe_cluster': {'enabled': True, 'n_clusters_c': 6, 'n_clusters_g': 44}, 'fe_stats': {'enabled': False}, 'pca': {'enabled': False}, 'scaler': 'none', 'shuffle_cols': True, 'variance_reduction': {'enabled': True, 'threshold': 0.9585049261745544}, 'use_zero_pred_model': True}, {'activation': ('selu', 'swish', 'swish'), 'dropout': (0.7, 0.7, 0.3), 'hid_layer': (1152, 1152, 2048), 'init_non_scored_weights': False, 'label_smoothing': 0.0007000000000000001, 'learning_rate': 0.016, 'epochs': 500}, {'threshold': 0}, 29, 1)\n",
    "\n",
    "# term_model = DeepMultiLabelModelCV(deep_params, 'terms')\n",
    "\n",
    "# y_term = y_to_terms(pd.concat([y, y0], axis=1))\n",
    "# term_model.cv(X_p, y_term, y0, n_split=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero class prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:30:39.588095Z",
     "start_time": "2020-11-29T17:30:39.585741Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_p, X_test_p = preprocess_X(fe_params, X.copy(), X_test.copy())\n",
    "\n",
    "# zero_model = DeepMultiLabelModel(nn_params, 'zero')\n",
    "\n",
    "# y_zero = y_to_zero_class(y)\n",
    "# zero_model.cv(X_p, y_zero, y0, n_split=7, run_name='tune_nn_1')\n",
    "\n",
    "# zero_model.predict(X_test_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error class prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:30:40.902196Z",
     "start_time": "2020-11-29T17:30:40.899672Z"
    }
   },
   "outputs": [],
   "source": [
    "# fe_params, nn_params, _, seed, _ = (\n",
    "#     {'fe_cluster': {'enabled': True, 'n_clusters_c': 14, 'n_clusters_g': 39}, 'fe_stats': {'enabled': True}, 'pca': {'enabled': False}, 'scaler': 'quantile', 'shuffle_cols': True, 'variance_reduction': {'enabled': False}}, \n",
    "#     {'activation': ('elu', 'elu', 'elu', 'elu'), 'batch_norm': True, \n",
    "#      'dropout': (0.3, 0.3, 0.4, 0.3), 'epochs': 100, 'hid_layer': (512, 1024, 512, 2048), \n",
    "#      'init_non_scored_weights': False, 'label_smoothing': 0.0001380444271082826, \n",
    "#      'learning_rate': 0.4083831289327425, 'weight_norm': True,\n",
    "# #      'class_weight': {0: 1, 1: 7, 2: 6, 3: 7, 4: 7, 5: 5}\n",
    "#     }, \n",
    "#     {'zero_threshold': 0.9837308739197401}, 13, 2\n",
    "# )\n",
    "# seed_everything(seed)\n",
    "\n",
    "# X_p, X_test_p = preprocess_X(fe_params, X.copy(), X_test.copy())\n",
    "\n",
    "# error_model = DeepMultiLabelModel(nn_params, 'errors', verbose=0)\n",
    "\n",
    "# y_error = y_to_error_classes(y)\n",
    "# error_model.cv(X_p, y_error, y0, n_split=5)\n",
    "# loss_df = error_model.errors(X_p, y_error)\n",
    "# loss_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blender model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:30:42.069494Z",
     "start_time": "2020-11-29T17:30:41.882643Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p bundles\n",
    "BREAK_SCORE=0.019\n",
    "class ModelBlender(object):\n",
    "    def __init__(self, deep_params, blend_params, seed=42, verbose=0, preprocess_params=None, base_model=DeepMultiLabelModelTorch):\n",
    "        self.deep_params = deep_params\n",
    "\n",
    "        self.num_epochs = deep_params['epochs']\n",
    "        self.blend_params = blend_params\n",
    "        self.seed = seed\n",
    "        self.preprocess_params = preprocess_params\n",
    "\n",
    "        bmodel_param = deep_params.get('base_model')\n",
    "        if bmodel_param == 'torch':\n",
    "            base_model = DeepMultiLabelModelTorch\n",
    "        elif bmodel_param == 'keras':\n",
    "            base_model = DeepMultiLabelModelKeras\n",
    "        \n",
    "        zero_params = blend_params.get('zero_params') or deep_params\n",
    "        errors_params = blend_params.get('errors_params') or deep_params\n",
    "\n",
    "        self.model_main = DeepMultiLabelModelCV(deep_params, model_name='main', seed=seed, verbose=verbose, base_model=base_model)\n",
    "        self.model_zero = DeepMultiLabelModelCV(zero_params or deep_params, model_name='zero', seed=seed, verbose=verbose, base_model=base_model)\n",
    "        self.model_errors = DeepMultiLabelModelCV(errors_params or deep_params, model_name='errors', seed=seed, verbose=verbose, base_model=base_model)\n",
    "        self.classes = []\n",
    "        self.metrics = {}\n",
    "\n",
    "    @property\n",
    "    def definition(self):\n",
    "        return self.model_main.definition\n",
    "\n",
    "    def predict(self, X):        \n",
    "        y = self.model_main.predict(X)\n",
    "        # Если выставлено, то принудительно ставим 0 везде, где модель предсказания 0 уверена\n",
    "        zero_pred_threshold = self.blend_params.get('zero_threshold', 0)\n",
    "        if zero_pred_threshold > 0:\n",
    "            zero_preds = self.model_zero.predict(X)[:, 0]\n",
    "            override_ind = zero_preds > zero_pred_threshold\n",
    "            print('Override to zeros: {} rows'.format(len(override_ind[override_ind])))\n",
    "            y[override_ind, :] = 0.\n",
    "\n",
    "        if self.blend_params.get('use_error_class'):\n",
    "            error_class_indices = [self.classes.index(e) for e in error_classes]\n",
    "            y[:, error_class_indices] = self.model_errors.predict(X)\n",
    "        return y\n",
    "    \n",
    "    def cv(self, X, y, y0, run_name=None, n_split=7, \n",
    "           metric_fn=log_loss_metric, overfit=True, \n",
    "           run_tags=None, return_pred=True\n",
    "          ):\n",
    "        seed_everything(self.seed)\n",
    "        \n",
    "        y_te = y.drop(columns=['drug_id'], errors='ignore')\n",
    "        ycols = list(y_te.columns)\n",
    "        self.classes = ycols\n",
    "        \n",
    "        preds = {}\n",
    "        _, y_pred = self.model_main.cv(\n",
    "            X, y, y0, \n",
    "            n_split=n_split, metric_fn=metric_fn, overfit=overfit,\n",
    "            run_tags=run_tags, run_name=run_name, return_pred=True, max_score=BREAK_SCORE\n",
    "        )\n",
    "        if y_pred is None:\n",
    "            if return_pred:\n",
    "                return None, None\n",
    "            return None\n",
    "\n",
    "        self.metrics['initial'] = metric_fn(y_te, y_pred)\n",
    "        preds['initial'] = y_pred\n",
    "\n",
    "        print('INITIAL', self.metrics['initial'])\n",
    "        zero_threshold = self.blend_params.get('zero_threshold', 0)\n",
    "        if zero_threshold > 0:\n",
    "            _, y_pred_zero = self.model_zero.cv(\n",
    "                X, y_to_zero_class(y_te), y_to_zero_class(y0), \n",
    "                n_split=n_split, metric_fn=metric_fn, overfit=overfit,\n",
    "                run_tags=run_tags, run_name=run_name, return_pred=True\n",
    "            )\n",
    "            zero_preds = y_pred_zero.iloc[:, 0]\n",
    "            override_ind = zero_preds > zero_threshold\n",
    "            print('Override to zeros: {} rows'.format(len(override_ind[override_ind])))\n",
    "            y_pred.loc[override_ind, :] = 0.\n",
    "            self.metrics['after_zero'] = metric_fn(y_te, y_pred)\n",
    "            preds['after_zero'] = y_pred\n",
    "            print('AFTER ZERO', self.metrics['after_zero'])\n",
    "        \n",
    "        if self.blend_params.get('use_error_class'):\n",
    "            _, y_pred_errors = self.model_errors.cv(\n",
    "                X, y_to_error_classes(y_te), y0, \n",
    "                n_split=n_split, metric_fn=metric_fn, overfit=overfit,\n",
    "                run_tags=run_tags, run_name=run_name, return_pred=True\n",
    "            )\n",
    "            y_pred.loc[:, error_classes] = y_pred_errors.loc[:, error_classes]\n",
    "            self.metrics['after_error'] = metric_fn(y_te, y_pred)\n",
    "            preds['after_error'] = y_pred\n",
    "            print('AFTER ERROR', self.metrics['after_error'])\n",
    "        \n",
    "        logloss_valid = metric_fn(y_te, y_pred)\n",
    "        self.metrics['final'] = logloss_valid\n",
    "        preds['final'] = y_pred\n",
    "\n",
    "        if has_internet and run_name:\n",
    "            bundle = {\n",
    "                'seed': self.seed,\n",
    "                'n_split': n_split,\n",
    "                'p_min': p_min,\n",
    "                'p_max': p_max,\n",
    "                'preprocess_params': self.preprocess_params,\n",
    "                'deep_params': self.deep_params,\n",
    "                'blend_params': self.blend_params,\n",
    "                'logloss_valid': logloss_valid,\n",
    "                'metrics': self.metrics,\n",
    "                'model_def': self.definition,\n",
    "                'run_name': run_name,\n",
    "                'run_tags': run_tags,\n",
    "                'predictions': preds\n",
    "            }\n",
    "            bundle_path = os.path.join('bundles', f'{int(time())}.pickle')\n",
    "            with open(bundle_path, 'wb') as fbundle:\n",
    "                pickle.dump(bundle, fbundle)\n",
    "            print(f'Write bundle: {bundle_path}')\n",
    "\n",
    "            mlflow.set_experiment('Kaggle-MOA-Blend')\n",
    "            with mlflow.start_run(run_name=run_name):\n",
    "                mlflow.log_params(dict_flatten({\n",
    "                    'seed': self.seed,\n",
    "                    'n_split': n_split,\n",
    "                    'p_min': p_min,\n",
    "                    'p_max': p_max,\n",
    "                    'preprocess_params': self.preprocess_params,\n",
    "                    'deep_params': self.deep_params,\n",
    "                    'blend_params': self.blend_params\n",
    "                }))\n",
    "                mlflow.log_metric(key=\"logloss_valid\", value=logloss_valid)\n",
    "                mlflow.log_metrics(self.metrics)\n",
    "                tags = {\n",
    "                    'model_def': self.definition,\n",
    "                    'run': run_name,\n",
    "                    'bundle_path': bundle_path\n",
    "                }\n",
    "                tags.update(run_tags)\n",
    "                mlflow.set_tags(tags)\n",
    "        if return_pred:\n",
    "            return logloss_valid, y_pred\n",
    "        return logloss_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:30:42.679642Z",
     "start_time": "2020-11-29T17:30:42.671113Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_context = {}\n",
    "\n",
    "def create_and_evaluate_model(\n",
    "    args, models=None, predictions=None, predictions_cv=None,\n",
    "    run_name='', n_split=7, verbose=0, overfit=False\n",
    "):\n",
    "    print(args)\n",
    "    fe_params, deep_params, blend_params, seed, y_quantiles = args\n",
    "\n",
    "    try:\n",
    "        X_p, X_test_p, y_p, y0_p = preprocess_X(fe_params, X.copy(), X_test.copy(), y.copy(), y0.copy(), seed=seed)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0.19\n",
    "\n",
    "    model = ModelBlender(deep_params, blend_params, seed=seed, verbose=verbose, preprocess_params=fe_params)\n",
    "    evaluate_context['current_iter'] = {\n",
    "        'model': model\n",
    "    }\n",
    "    logloss_valid, y_pred = model.cv(\n",
    "        X_p, y_p, y0_p, n_split=n_split, overfit=overfit, \n",
    "        run_name=run_name, run_tags={'args': str(args)}, return_pred=True\n",
    "    )\n",
    "    if logloss_valid is None:\n",
    "        return 0.19\n",
    "\n",
    "    if models is not None:\n",
    "        models.append(model)\n",
    "\n",
    "    if predictions is not None:\n",
    "        predictions.append(model.predict(X_test_p))\n",
    "    \n",
    "    if predictions_cv is not None:\n",
    "        predictions_cv.append(y_pred)\n",
    "\n",
    "    evaluate_context['last_iter'] = {\n",
    "        'model': model,\n",
    "        'logloss': logloss_valid\n",
    "    }\n",
    "\n",
    "    print(f'Final valid logloss: {logloss_valid}')\n",
    "    return logloss_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:30:43.700032Z",
     "start_time": "2020-11-29T17:30:43.696907Z"
    }
   },
   "outputs": [],
   "source": [
    "p_min = 1e-5\n",
    "p_max = 1.-1e-5\n",
    "batch_size = 128\n",
    "fold = fold_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T16:04:56.099785Z",
     "start_time": "2020-11-29T10:51:11.909264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'fe_cluster': {'enabled': True, 'n_clusters_c': 6, 'n_clusters_g': 44}, 'fe_stats': {'enabled': True}, 'pca': {'enabled': True, 'n_comp_cells': 60, 'n_comp_genes': 463}, 'scaler': 'quantile', 'shuffle_cols': False, 'variance_reduction': {'enabled': True, 'threshold': 0.9}}, {'base_model': 'torch', 'batch_norm': True, 'weight_norm': True, 'activation': ['leaky_relu', 'leaky_relu'], 'dropout': [0.25, 0.25], 'hid_layer': [1500, 1500], 'init_non_scored_weights': False, 'label_smoothing': 0, 'learning_rate': 0.001, 'epochs': 35}, {'use_error_class': False, 'zero_threshold': 0}, 0, 1)\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.3858600500402332, valid_loss: 0.021289364519444378\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.01679550291159908, valid_loss: 0.01718774480237202\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.015685777627784787, valid_loss: 0.016533557325601578\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.012344526034559457, valid_loss: 0.016207189637828957\n",
      "[00:26], Fold 0: 0.016158695520533846\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.3855497151300425, valid_loss: 0.021839545108377934\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.01685767053477182, valid_loss: 0.01711569984697483\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.01576673939027859, valid_loss: 0.01633428329263221\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.012383133478690194, valid_loss: 0.015960189302197912\n",
      "[00:24], Fold 1: 0.016323172243829908\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.38564713727256267, valid_loss: 0.020926218564537438\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.01679764633984056, valid_loss: 0.017265769737687977\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.015830593325823317, valid_loss: 0.01661734930662946\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.012526993540236514, valid_loss: 0.016207592053846878\n",
      "[00:25], Fold 2: 0.01607562892072742\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.3864626481787849, valid_loss: 0.0211462832309983\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.016905884421963727, valid_loss: 0.01704054507850246\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.015831048116941034, valid_loss: 0.01644798774610866\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.012525334428171164, valid_loss: 0.01612175488844514\n",
      "[00:25], Fold 3: 0.01606447114574304\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.386004718162745, valid_loss: 0.02095357235521078\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.016795768843295464, valid_loss: 0.017539600164375523\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.015712431475577007, valid_loss: 0.016614150171252815\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.012391641124627973, valid_loss: 0.016110739670693874\n",
      "[00:25], Fold 4: 0.016003872635627272\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.3858246816222677, valid_loss: 0.021222937767478554\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.01687051975772581, valid_loss: 0.01704768400469964\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.015794342397960997, valid_loss: 0.01639699279753999\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.012410354309978376, valid_loss: 0.015992748085409403\n",
      "[00:25], Fold 5: 0.016165505448370296\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.3859262950149656, valid_loss: 0.02152838371694088\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.01680863366412525, valid_loss: 0.017134903210469267\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.015712616538614717, valid_loss: 0.016538250260055065\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.012197997115324927, valid_loss: 0.016182375436818056\n",
      "[00:23], Fold 6: 0.016011906287759097\n",
      "FOLD: 7, EPOCH: 0, train_loss: 0.38807638383378057, valid_loss: 0.020981521853669124\n",
      "FOLD: 7, EPOCH: 5, train_loss: 0.0168463381590279, valid_loss: 0.017288843804801054\n",
      "FOLD: 7, EPOCH: 10, train_loss: 0.01579362837924757, valid_loss: 0.01667931197549809\n",
      "FOLD: 7, EPOCH: 15, train_loss: 0.0125105066210714, valid_loss: 0.016270896631546995\n",
      "[00:25], Fold 7: 0.01619633801881288\n",
      "FOLD: 8, EPOCH: 0, train_loss: 0.38608628790114674, valid_loss: 0.02109396474605257\n",
      "FOLD: 8, EPOCH: 5, train_loss: 0.016849029098303263, valid_loss: 0.01747656245292588\n",
      "FOLD: 8, EPOCH: 10, train_loss: 0.015719742613723716, valid_loss: 0.016648116817867212\n",
      "FOLD: 8, EPOCH: 15, train_loss: 0.012252083901344364, valid_loss: 0.0161728795025159\n",
      "[00:23], Fold 8: 0.01592992488886017\n",
      "Valid logloss: 0.016103282552900118\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mlflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c8958bae3930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpredictions_cv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_preds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mn_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSPLITS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'base_{baseline_num}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         )\n",
      "\u001b[0;32m<ipython-input-16-053d4d542d35>\u001b[0m in \u001b[0;36mcreate_and_evaluate_model\u001b[0;34m(args, models, predictions, predictions_cv, run_name, n_split, verbose, overfit)\u001b[0m\n\u001b[1;32m     20\u001b[0m     logloss_valid, y_pred = model.cv(\n\u001b[1;32m     21\u001b[0m         \u001b[0mX_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverfit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'args'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogloss_valid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-93c09b10eda7>\u001b[0m in \u001b[0;36mcv\u001b[0;34m(self, X, y, y0, run_name, n_split, metric_fn, overfit, run_tags, return_pred)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mn_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverfit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverfit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mrun_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBREAK_SCORE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         )\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-badcb34867f2>\u001b[0m in \u001b[0;36mcv\u001b[0;34m(self, X, y, y0, run_name, n_split, return_pred, metric_fn, overfit, run_tags, max_score)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Valid logloss: {logloss_valid}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_internet\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kaggle-MOA-{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 mlflow.log_params(dict_flatten({\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlflow' is not defined"
     ]
    }
   ],
   "source": [
    "# Сюда записываем все доступные предсказания для блендинга\n",
    "from copy import deepcopy\n",
    "\n",
    "fold = fold_simple\n",
    "baseline_num = 2\n",
    "EPOCHS = 16\n",
    "SPLITS = 9\n",
    "# EPOCHS = 3\n",
    "# SPLITS = 3\n",
    "BREAK_SCORE = 0.19\n",
    "cv_preds = []\n",
    "predictions_final = []\n",
    "\n",
    "final_conf = [\n",
    "    # ({'fe_cluster': {'enabled': True, 'n_clusters_c': 6, 'n_clusters_g': 44}, 'fe_stats': {'enabled': False}, 'pca': {'enabled': False}, 'scaler': 'none', 'shuffle_cols': False, 'variance_reduction': {'enabled': True, 'threshold': 0.9585049261745544}}, {'activation': ('leaky_relu', 'leaky_relu', 'leaky_relu'), 'batch_norm': True, 'dropout': (0.7, 0.7, 0.3), 'epochs': 35, 'hid_layer': (1152, 1152, 2048), 'init_non_scored_weights': False, 'label_smoothing': 0.0007000000000000001, 'learning_rate': 0.016, 'weight_norm': True}, {'use_error_class': False, 'zero_threshold': 0.}, 29, 1),\n",
    "    # public 1 0.017286926515769126\n",
    "    (\n",
    "        {\n",
    "            'fe_cluster': {'enabled': False, 'n_clusters_c': 6, 'n_clusters_g': 44}, \n",
    "            'fe_stats': {'enabled': False}, \n",
    "            'pca': {'enabled': True, 'n_comp_cells': 50, 'n_comp_genes': 600}, \n",
    "            'scaler': 'quantile', 'shuffle_cols': False, \n",
    "            'variance_reduction': {'enabled': True, 'threshold': 0.8}\n",
    "        },\n",
    "        {\n",
    "            'batch_norm': True, 'weight_norm': True, \n",
    "            'activation': ['leaky_relu', 'leaky_relu', 'leaky_relu', 'leaky_relu'], \n",
    "            'dropout': [0.3, 0.3, 0.4, 0.3], \n",
    "            'hid_layer': [512, 1024, 512, 2048], \n",
    "            'init_non_scored_weights': True,\n",
    "            'label_smoothing': 0.0015, \n",
    "            'learning_rate': 0.001, 'epochs': 35,\n",
    "            'base_model': 'keras'\n",
    "        },\n",
    "        {\n",
    "            'use_error_class': False, 'zero_threshold': False\n",
    "        },\n",
    "        8, 1\n",
    "    ),\n",
    "    # public 2\n",
    "    ({\n",
    "        'fe_cluster': {'enabled': False, 'n_clusters_c': 6, 'n_clusters_g': 44}, \n",
    "        'fe_stats': {'enabled': False}, \n",
    "        'pca': {'enabled': True, 'n_comp_cells': 60, 'n_comp_genes': 463}, \n",
    "        'scaler': 'quantile', 'shuffle_cols': False, \n",
    "        'variance_reduction': {'enabled': True, 'threshold': 0.9}\n",
    "    },\n",
    "    {\n",
    "        'batch_norm': True, 'weight_norm': True, \n",
    "        'activation': ['leaky_relu', 'leaky_relu'], \n",
    "        'dropout': [0.25, 0.25], \n",
    "        'hid_layer': [1500, 1500], \n",
    "        'init_non_scored_weights': False, \n",
    "        'label_smoothing': 0.001,\n",
    "        'learning_rate': 0.001, 'epochs': 35,\n",
    "        'base_model': 'torch'\n",
    "    },\n",
    "    {\n",
    "        'use_error_class': False, 'zero_threshold': 0\n",
    "    },\n",
    "    0, 1\n",
    "    ),\n",
    "]\n",
    "\n",
    "final_conf = [\n",
    "# public 3\n",
    "#     ({\n",
    "#         'fe_cluster': {'enabled': True, 'n_clusters_c': 4, 'n_clusters_g': 22}, \n",
    "#         'fe_stats': {'enabled': True}, \n",
    "#         'pca': {'enabled': True, 'n_comp_cells': 50, 'n_comp_genes': 600, 'n_clusters': 5},\n",
    "#         'scaler': 'quantile', 'shuffle_cols': False, \n",
    "#         'variance_reduction': {'enabled': True, 'threshold': 0.85}\n",
    "#     }, {\n",
    "#         'batch_norm': True, 'weight_norm': 'output',\n",
    "#         'activation': ['leaky_relu', 'leaky_relu', 'leaky_relu', 'leaky_relu'], \n",
    "#         'dropout': [0.5, 0.35, 0.3, 0.25], \n",
    "#         'hid_layer': [1500, 1250, 1000, 750], \n",
    "#         'init_non_scored_weights': 'ALL_TARGETS',\n",
    "#         'label_smoothing': 0.001, \n",
    "#         'learning_rate': 1e-3, 'epochs': 25, 'base_model': 'torch'\n",
    "#     }, {'use_error_class': False, 'zero_threshold': 0}, 0, 1),\n",
    "    \n",
    "#     ({'fe_cluster': {'enabled': False, 'n_clusters_c': 6, 'n_clusters_g': 44}, 'fe_stats': {'enabled': False}, 'pca': {'enabled': True, 'n_comp_cells': 50, 'n_comp_genes': 600}, 'scaler': 'quantile', 'shuffle_cols': True, 'variance_reduction': {'enabled': True, 'threshold': 0.8}}, {'base_model': 'keras', 'batch_norm': True, 'weight_norm': True, 'activation': ['elu', 'elu', 'elu', 'elu'], 'dropout': [0.3, 0.3, 0.4, 0.3], 'hid_layer': [512, 1024, 512, 2048], 'init_non_scored_weights': False, 'label_smoothing': 0.0015, 'learning_rate': 0.001, 'epochs': 35}, {'use_error_class': False, 'zero_threshold': 0}, 8, 1), \n",
    "#     ({'fe_cluster': {'enabled': True, 'n_clusters_c': 4, 'n_clusters_g': 22}, 'fe_stats': {'enabled': True}, 'pca': {'enabled': True, 'n_comp_cells': 50, 'n_comp_genes': 600, 'n_clusters': 5}, 'scaler': 'quantile', 'shuffle_cols': False, 'variance_reduction': {'enabled': True, 'threshold': 0.85}}, {'base_model': 'keras', 'batch_norm': True, 'weight_norm': 'output', 'activation': ['leaky_relu', 'leaky_relu', 'leaky_relu', 'leaky_relu'], 'dropout': [0.5, 0.35, 0.3, 0.25], 'hid_layer': [1500, 1250, 1000, 750], 'init_non_scored_weights': True, 'label_smoothing': 0, 'learning_rate': 0.001, 'epochs': 25}, {'use_error_class': False, 'zero_threshold': 0}, 42, 1), \n",
    "\n",
    "    # 0.016096763585134236\n",
    "    # ({'fe_cluster': {'enabled': False, 'n_clusters_c': 6, 'n_clusters_g': 44}, 'fe_stats': {'enabled': False}, 'pca': {'enabled': True, 'n_comp_cells': 60, 'n_comp_genes': 463}, 'scaler': 'quantile', 'shuffle_cols': False, 'variance_reduction': {'enabled': True, 'threshold': 0.9}}, {'base_model': 'torch', 'batch_norm': True, 'weight_norm': True, 'activation': ['leaky_relu', 'leaky_relu'], 'dropout': [0.25, 0.25], 'hid_layer': [1500, 1500], 'init_non_scored_weights': False, 'label_smoothing': 0.001, 'learning_rate': 0.001, 'epochs': 35}, {'use_error_class': False, 'zero_threshold': 0}, 0, 1),\n",
    "    # 0.016103282552900118\n",
    "    ({'fe_cluster': {'enabled': True, 'n_clusters_c': 6, 'n_clusters_g': 44}, 'fe_stats': {'enabled': True}, 'pca': {'enabled': True, 'n_comp_cells': 60, 'n_comp_genes': 463}, 'scaler': 'quantile', 'shuffle_cols': False, 'variance_reduction': {'enabled': True, 'threshold': 0.9}}, {'base_model': 'torch', 'batch_norm': True, 'weight_norm': True, 'activation': ['leaky_relu', 'leaky_relu'], 'dropout': [0.25, 0.25], 'hid_layer': [1500, 1500], 'init_non_scored_weights': False, 'label_smoothing': 0, 'learning_rate': 0.001, 'epochs': 35}, {'use_error_class': False, 'zero_threshold': 0}, 0, 1),\n",
    "]\n",
    "\n",
    "for cv_seed in [None]:\n",
    "    for conf in final_conf:\n",
    "        conf2 = deepcopy(conf)\n",
    "        if cv_seed:\n",
    "            conf2 = list(conf2)\n",
    "            conf2[3] = cv_seed\n",
    "            conf2 = tuple(conf2)\n",
    "        create_and_evaluate_model(\n",
    "            conf2,\n",
    "            predictions=predictions_final,\n",
    "            predictions_cv=cv_preds,\n",
    "            n_split=SPLITS,\n",
    "            run_name=f'base_{baseline_num}'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yres = y.drop(columns=['drug_id'], errors='ignore')\n",
    "yres = yres[X['cp_type'] == 0].reset_index(drop = True)\n",
    "ycols = yres.columns\n",
    "\n",
    "def print_final():\n",
    "    print('\\nFinal results:')\n",
    "    prev_CV = 0.015973\n",
    "    cv_res = []\n",
    "    for i, cv_pred in enumerate(cv_preds):\n",
    "        print(f'Logloss {i}:', log_loss_metric(yres, cv_pred))\n",
    "        cv_res.append(cv_pred.values)\n",
    "\n",
    "    final_pred = y_arr_to_df(np.mean(cv_res, axis=0), ycols)\n",
    "    print(f'CV: {log_loss_metric(yres, final_pred)}, prev: {prev_CV}')\n",
    "\n",
    "print_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cv_seed in [9]:\n",
    "    for conf in final_conf:\n",
    "        conf2 = deepcopy(conf)\n",
    "        if cv_seed:\n",
    "            conf2 = list(conf2)\n",
    "            conf2[3] = cv_seed\n",
    "            conf2 = tuple(conf2)\n",
    "        create_and_evaluate_model(\n",
    "            conf2,\n",
    "            predictions=predictions_final,\n",
    "            predictions_cv=cv_preds,\n",
    "            n_split=SPLITS,\n",
    "            run_name=f'base_{baseline_num}'\n",
    "        )\n",
    "print_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T11:07:56.368963Z",
     "start_time": "2020-11-28T11:07:55.503743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Формируем предсказания на основе множества предсказаний\n",
    "# Сюда записываем submission предсказания\n",
    "df_sample.loc[:, ycols] = 0\n",
    "df_sample.loc[:, ycols] = np.mean(predictions_final, axis=0)\n",
    "# У ctl_vehicle все классы - 0, поэтому просто зануляем\n",
    "# Правильно ли?\n",
    "df_sample.loc[ind_te, ycols] = 0\n",
    "display(df_sample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T11:09:02.152135Z",
     "start_time": "2020-11-28T11:09:00.703708Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sample.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
